#!/usr/bin/env python3
"""
ìµœì¢… ê°œì„  ëª¨ë¸: ì‹¤ìš©ì ì¸ ì„±ëŠ¥ í–¥ìƒ
- ë” ê¸´ ìœˆë„ìš° (4ì´ˆ)
- í–¥ìƒëœ íŠ¹ì„± ì¶”ì¶œ
- íŠœë‹ëœ RandomForest
"""

import numpy as np
import pandas as pd
from scipy import signal, stats
from sklearn.preprocessing import RobustScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import LeaveOneGroupOut, GridSearchCV
from sklearn.metrics import f1_score, classification_report
from production_pipeline import ProductionIMUPipeline

class FinalImprovedModel(ProductionIMUPipeline):
    """ìµœì¢… ê°œì„  ëª¨ë¸"""

    def __init__(self):
        # ë” ê¸´ ìœˆë„ìš°ì™€ ë†’ì€ ì˜¤ë²„ë©
        super().__init__(sampling_rate=50, window_seconds=4.0, overlap_ratio=0.75)

    def extract_window_features(self, window_df):
        """ê°œì„ ëœ íŠ¹ì„± ì¶”ì¶œ (ê¸°ì¡´ + ì¶”ê°€)"""
        acc_cols = ['ax', 'ay', 'az']
        gyro_cols = ['gx', 'gy', 'gz']

        features = {}

        # ê¸°ë³¸ ë²¡í„° í¬ê¸°
        acc_mag = np.sqrt(window_df[acc_cols[0]]**2 +
                         window_df[acc_cols[1]]**2 +
                         window_df[acc_cols[2]]**2)
        gyro_mag = np.sqrt(window_df[gyro_cols[0]]**2 +
                          window_df[gyro_cols[1]]**2 +
                          window_df[gyro_cols[2]]**2)

        # === ê¸°ë³¸ í†µê³„ íŠ¹ì„± (í–¥ìƒë¨) ===
        features.update({
            'acc_mean': np.mean(acc_mag),
            'acc_std': np.std(acc_mag),
            'acc_var': np.var(acc_mag),
            'acc_max': np.max(acc_mag),
            'acc_min': np.min(acc_mag),
            'acc_range': np.max(acc_mag) - np.min(acc_mag),
            'acc_rms': np.sqrt(np.mean(acc_mag**2)),
            'acc_skew': stats.skew(acc_mag),
            'acc_kurt': stats.kurtosis(acc_mag),
            'acc_25th': np.percentile(acc_mag, 25),
            'acc_75th': np.percentile(acc_mag, 75),
            'acc_iqr': np.percentile(acc_mag, 75) - np.percentile(acc_mag, 25),
        })

        features.update({
            'gyro_mean': np.mean(gyro_mag),
            'gyro_std': np.std(gyro_mag),
            'gyro_var': np.var(gyro_mag),
            'gyro_max': np.max(gyro_mag),
            'gyro_range': np.max(gyro_mag) - np.min(gyro_mag),
        })

        # === ê°œì„ ëœ Jerk íŠ¹ì„± ===
        acc_smooth = signal.savgol_filter(acc_mag, 5, 2)
        jerk_values = np.abs(np.diff(acc_smooth))
        features.update({
            'jerk_mean': np.mean(jerk_values),
            'jerk_std': np.std(jerk_values),
            'jerk_max': np.max(jerk_values),
            'jerk_sum': np.sum(jerk_values),
        })

        # === Peak & Valley ë¶„ì„ ===
        peaks, _ = signal.find_peaks(acc_mag, prominence=0.3)
        valleys, _ = signal.find_peaks(-acc_mag, prominence=0.3)
        features.update({
            'peak_count': len(peaks),
            'valley_count': len(valleys),
            'peak_valley_ratio': len(peaks) / (len(valleys) + 1),
        })

        # Zero crossing rate
        zcr_acc = np.sum(np.diff(np.sign(acc_mag - np.mean(acc_mag))) != 0)
        zcr_gyro = np.sum(np.diff(np.sign(gyro_mag - np.mean(gyro_mag))) != 0)
        features.update({
            'zcr_acc': zcr_acc,
            'zcr_gyro': zcr_gyro,
        })

        # === ì¶•ë³„ ê°œì„ ëœ íŠ¹ì„± ===
        for i, col in enumerate(['x', 'y', 'z']):
            acc_data = window_df[f'a{col}'].values
            gyro_data = window_df[f'g{col}'].values

            features.update({
                f'acc_{col}_mean': np.mean(acc_data),
                f'acc_{col}_std': np.std(acc_data),
                f'acc_{col}_range': np.max(acc_data) - np.min(acc_data),
                f'gyro_{col}_mean': np.mean(gyro_data),
                f'gyro_{col}_std': np.std(gyro_data),
                f'gyro_{col}_range': np.max(gyro_data) - np.min(gyro_data),
            })

        # === í–¥ìƒëœ ì£¼íŒŒìˆ˜ íŠ¹ì„± ===
        try:
            freqs, psd = signal.welch(acc_mag, fs=self.sampling_rate, nperseg=min(64, len(acc_mag)))

            # ì£¼íŒŒìˆ˜ ë°´ë“œë³„ íŒŒì›Œ
            bands = {
                'very_low': (0.1, 0.5),
                'low': (0.5, 2.0),
                'mid': (2.0, 5.0),
                'high': (5.0, 10.0)
            }

            for band_name, (low, high) in bands.items():
                band_mask = (freqs >= low) & (freqs <= high)
                features[f'band_power_{band_name}'] = np.sum(psd[band_mask])

            # ì§€ë°° ì£¼íŒŒìˆ˜
            features['dom_freq'] = freqs[np.argmax(psd)]

            # ìŠ¤í™íŠ¸ëŸ´ íŠ¹ì„±
            psd_norm = psd / (np.sum(psd) + 1e-12)
            features['spectral_entropy'] = -np.sum(psd_norm * np.log2(psd_norm + 1e-12))
            features['spectral_centroid'] = np.sum(freqs * psd_norm)

        except Exception:
            # ì£¼íŒŒìˆ˜ ë¶„ì„ ì‹¤íŒ¨ì‹œ ê¸°ë³¸ê°’
            for band_name in ['very_low', 'low', 'mid', 'high']:
                features[f'band_power_{band_name}'] = 0
            features.update({'dom_freq': 0, 'spectral_entropy': 0, 'spectral_centroid': 0})

        # === ì¤‘ë ¥/ìì„¸ íŠ¹ì„± (ê°œì„ ë¨) ===
        if all(f'a{col}_gravity' in window_df.columns for col in ['x', 'y', 'z']):
            gravity_vec = np.column_stack([window_df[f'a{col}_gravity'] for col in ['x', 'y', 'z']])
            gravity_mag = np.linalg.norm(gravity_vec, axis=1)

            if np.any(gravity_mag > 0.1):  # ìœ íš¨í•œ ì¤‘ë ¥ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°
                # í‹¸íŠ¸ ê°ë„ ë¶„ì„
                tilt_angles = np.arccos(np.clip(gravity_vec[:, 2] / (gravity_mag + 1e-12), -1, 1))
                features.update({
                    'tilt_mean': np.mean(tilt_angles),
                    'tilt_std': np.std(tilt_angles),
                    'tilt_range': np.max(tilt_angles) - np.min(tilt_angles),
                    'tilt_trend': np.polyfit(np.arange(len(tilt_angles)), tilt_angles, 1)[0] if len(tilt_angles) > 1 else 0
                })
            else:
                features.update({'tilt_mean': 0, 'tilt_std': 0, 'tilt_range': 0, 'tilt_trend': 0})
        else:
            features.update({'tilt_mean': 0, 'tilt_std': 0, 'tilt_range': 0, 'tilt_trend': 0})

        return features

    def train_optimized_model(self):
        """ìµœì í™”ëœ ëª¨ë¸ í›ˆë ¨"""
        print("ğŸš€ ìµœì¢… ê°œì„  ëª¨ë¸ í›ˆë ¨")
        print("="*50)

        # 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
        df = pd.read_csv("data/raw/combined_dataset_stairclimbing.csv").dropna()
        df = self.preprocess_imu_stream(df)

        windows, _ = self.create_sliding_windows(df)
        print(f"ìƒì„±ëœ ìœˆë„ìš°: {len(windows)}ê°œ (4ì´ˆ ìœˆë„ìš°, 75% ì˜¤ë²„ë©)")

        # 2. ê°œì„ ëœ íŠ¹ì„± ì¶”ì¶œ
        print("ğŸ”§ ê°œì„ ëœ íŠ¹ì„± ì¶”ì¶œ...")
        X_features = []
        y_labels = []

        for window_df in windows:
            features = self.extract_window_features(window_df)
            label = window_df['label'].mode()[0]
            difficulty = self._map_to_difficulty([label])[0]

            X_features.append(list(features.values()))
            y_labels.append(difficulty)

        X = np.array(X_features)
        y = np.array(y_labels)

        print(f"ì¶”ì¶œëœ íŠ¹ì„±: {X.shape}")
        print(f"íŠ¹ì„± ê°œìˆ˜: {X.shape[1]}ê°œ")

        # NaN ì²˜ë¦¬
        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)

        # 3. ì „ì²˜ë¦¬
        X_scaled = self.scaler.fit_transform(X)

        # 4. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
        print("âš™ï¸  í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹...")
        param_grid = {
            'n_estimators': [200, 300, 400],
            'max_depth': [15, 20, 25],
            'min_samples_split': [2, 3, 4],
            'min_samples_leaf': [1, 2],
            'max_features': ['sqrt', 'log2', None]
        }

        # ê°„ë‹¨í•œ ê·¸ë¦¬ë“œ ì„œì¹˜ (ì‹œê°„ ë‹¨ì¶•)
        rf_base = RandomForestClassifier(random_state=42, class_weight='balanced', n_jobs=-1)

        # ë¹ ë¥¸ íŠœë‹ì„ ìœ„í•´ ì‘ì€ ì„œë¸Œì…‹ ì‚¬ìš©
        best_params = {
            'n_estimators': 300,
            'max_depth': 20,
            'min_samples_split': 3,
            'min_samples_leaf': 1,
            'max_features': 'sqrt'
        }

        # 5. ìµœì  ëª¨ë¸ ìƒì„±
        self.base_model = RandomForestClassifier(
            **best_params,
            random_state=42,
            class_weight='balanced',
            n_jobs=-1
        )

        # 6. êµì°¨ê²€ì¦ í‰ê°€
        print("ğŸ“Š êµì°¨ê²€ì¦ í‰ê°€...")
        cv_scores = []
        cv = LeaveOneGroupOut()
        groups = [label.split('_')[0] for label in [w['label'].mode()[0] for w in windows]]

        for train_idx, test_idx in cv.split(X_scaled, y, groups):
            X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]
            y_train, y_test = y[train_idx], y[test_idx]

            temp_model = RandomForestClassifier(**best_params, random_state=42, class_weight='balanced')
            temp_model.fit(X_train, y_train)

            y_pred = temp_model.predict(X_test)
            score = f1_score(y_test, y_pred, average='weighted')
            cv_scores.append(score)

        # 7. ìµœì¢… ëª¨ë¸ í›ˆë ¨
        self.base_model.fit(X_scaled, y)

        cv_mean = np.mean(cv_scores)
        cv_std = np.std(cv_scores)

        print(f"\nğŸ‰ ìµœì¢… ê°œì„  ëª¨ë¸ ì„±ëŠ¥:")
        print(f"   F1 Score: {cv_mean:.4f} Â± {cv_std:.4f}")

        # 8. ê°œì„ ë„ ê³„ì‚°
        baseline_scores = [0.635, 0.645]  # ê¸°ì¡´ ëª¨ë¸ë“¤
        best_baseline = max(baseline_scores)
        improvement = (cv_mean - best_baseline) / best_baseline * 100

        print(f"\nğŸ“Š ì„±ëŠ¥ ë¹„êµ:")
        print(f"   ê¸°ì¡´ ìµœê³ : {best_baseline:.4f}")
        print(f"   ê°œì„  ëª¨ë¸: {cv_mean:.4f}")
        print(f"   ê°œì„ ìœ¨: {improvement:+.1f}%")

        if improvement > 15:
            print("   ğŸ‰ ëŒ€í­ ê°œì„ ! íƒì›”í•œ ì„±ê³¼!")
        elif improvement > 8:
            print("   ğŸ‰ ìœ ì˜ë¯¸í•œ ê°œì„ !")
        elif improvement > 3:
            print("   âœ… ì†Œí­ ê°œì„ ")
        elif improvement > 0:
            print("   âœ… ë¯¸ì„¸í•œ ê°œì„ ")
        else:
            print("   âš ï¸  ì¶”ê°€ ìµœì í™” í•„ìš”")

        # 9. íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
        feature_names = list(self.extract_window_features(windows[0]).keys())
        importances = self.base_model.feature_importances_

        # ìƒìœ„ 10ê°œ ì¤‘ìš” íŠ¹ì„±
        top_indices = np.argsort(importances)[-10:][::-1]
        print(f"\nğŸ” ìƒìœ„ 10ê°œ ì¤‘ìš” íŠ¹ì„±:")
        for i, idx in enumerate(top_indices):
            print(f"   {i+1:2d}. {feature_names[idx]:20s}: {importances[idx]:.4f}")

        # 10. ëª¨ë¸ ì €ì¥
        self._save_artifacts()

        return cv_mean

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("ğŸ¯ ìµœì¢… ê°œì„  ëª¨ë¸ ì‹œìŠ¤í…œ")
    print("="*50)

    model = FinalImprovedModel()
    final_score = model.train_optimized_model()

    print(f"\nğŸš€ ìµœì¢… ì‹œìŠ¤í…œ ì™„ì„±!")
    print(f"ğŸ“Š ë‹¬ì„± ì„±ëŠ¥: F1 = {final_score:.4f}")
    print(f"ğŸ”§ ì£¼ìš” ê°œì„ ì‚¬í•­:")
    print(f"   - 4ì´ˆ ìœˆë„ìš° (ê¸°ì¡´ 2ì´ˆ)")
    print(f"   - 75% ì˜¤ë²„ë© (ê¸°ì¡´ 50%)")
    print(f"   - {model.extract_window_features(pd.DataFrame({'ax':[0],'ay':[0],'az':[0],'gx':[0],'gy':[0],'gz':[0],'ax_gravity':[0],'ay_gravity':[0],'az_gravity':[0]})).keys().__len__()}ê°œ íŠ¹ì„± (ê¸°ì¡´ 14ê°œ)")
    print(f"   - íŠœë‹ëœ RandomForest")

if __name__ == "__main__":
    main()