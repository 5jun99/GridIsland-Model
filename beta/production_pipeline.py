#!/usr/bin/env python3
"""
ì‹¤ì „ìš© IMU ê¸°ë°˜ ê²½ë¡œ ë‚œì´ë„ ì¶”ì • íŒŒì´í”„ë¼ì¸
ë‘ ë°ì´í„°ì…‹ ì—­í•  ë¶„ë‹´: HAR-PMD(ê¸°ì´ˆ í‘œí˜„) + Stairclimbing(ë‚œì´ë„ ì‹œë“œ)
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import numpy as np
import pandas as pd
from scipy import signal, stats
from scipy.fft import fft, fftfreq
from sklearn.preprocessing import RobustScaler
from sklearn.decomposition import PCA
from sklearn.cluster import HDBSCAN
from sklearn.mixture import GaussianMixture
from sklearn.model_selection import LeaveOneGroupOut
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, f1_score
import pickle
import warnings
warnings.filterwarnings('ignore')

class ProductionIMUPipeline:
    """ì‹¤ì „ìš© IMU íŒŒì´í”„ë¼ì¸"""

    def __init__(self, sampling_rate=50, window_seconds=2.0, overlap_ratio=0.5):
        self.sampling_rate = sampling_rate
        self.window_size = int(window_seconds * sampling_rate)  # 100 samples
        self.stride = int(self.window_size * (1 - overlap_ratio))  # 50 samples

        # ì „ì²˜ë¦¬ ì»´í¬ë„ŒíŠ¸
        self.scaler = RobustScaler()
        self.pca = None

        # ëª¨ë¸ ì»´í¬ë„ŒíŠ¸
        self.base_model = None
        self.difficulty_mapper = None

        # ë¼ë²¨ ë§¤í•‘
        self.difficulty_map = {
            0: "í‰ì§€ (Easy)",
            1: "ê²½ì‚¬ (Moderate)",
            2: "ê³„ë‹¨ (Hard)",
            3: "ê¸‰ê²½ì‚¬/ìž¥ì• ë¬¼ (Extreme)"
        }

        # ì—£ì§€ ë¹„ìš© ê³„ìˆ˜
        self.alpha = 1.0  # ê±°ë¦¬ ê°€ì¤‘ì¹˜
        self.beta = 2.0   # ë‚œì´ë„ ê°€ì¤‘ì¹˜
        self.gamma = 10.0 # ì œì•½ íŒ¨ë„í‹°

    def preprocess_imu_stream(self, df, gravity_compensation=True):
        """IMU ìŠ¤íŠ¸ë¦¼ ì „ì²˜ë¦¬"""
        print("ðŸ”§ IMU ë°ì´í„° ì „ì²˜ë¦¬...")

        # 1. 50Hz ë¦¬ìƒ˜í”Œë§ (í•„ìš”ì‹œ)
        if len(df) > 0:
            df = df.copy()

            # íƒ€ìž„ìŠ¤íƒ¬í”„ê°€ ìžˆë‹¤ë©´ í™œìš©
            if 'timestamp' in df.columns:
                df['timestamp'] = pd.to_datetime(df['timestamp'])
                df = df.set_index('timestamp').resample(f'{1000//self.sampling_rate}ms').mean()
                df = df.dropna().reset_index()

        # 2. ì„¼ì„œ ì»¬ëŸ¼ í™•ì¸
        acc_cols = ['ax', 'ay', 'az']
        gyro_cols = ['gx', 'gy', 'gz']

        required_cols = acc_cols + gyro_cols
        if not all(col in df.columns for col in required_cols):
            missing = [col for col in required_cols if col not in df.columns]
            raise ValueError(f"í•„ìˆ˜ ì„¼ì„œ ì»¬ëŸ¼ ëˆ„ë½: {missing}")

        # 3. ì¤‘ë ¥ ë³´ì • (ì˜µì…˜)
        if gravity_compensation:
            df = self._gravity_compensation(df, acc_cols)

        # 4. ëˆ„ë½ê°’ ì²˜ë¦¬ (ìœˆë„ìš° ë“œë¡­ ë°©ì‹)
        df = df.dropna(subset=required_cols)

        print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df)}ê°œ ìƒ˜í”Œ")
        return df

    def _gravity_compensation(self, df, acc_cols, cutoff_freq=0.3):
        """ì €ì—­í†µê³¼ í•„í„°ë¡œ ì¤‘ë ¥ ì„±ë¶„ ì œê±°"""
        sos = signal.butter(4, cutoff_freq, btype='low', fs=self.sampling_rate, output='sos')

        for col in acc_cols:
            # ì¤‘ë ¥ ì„±ë¶„ ì¶”ì •
            gravity = signal.sosfilt(sos, df[col])
            # ì¤‘ë ¥ ì œê±°í•œ ì„ í˜•ê°€ì†ë„
            df[f'{col}_linear'] = df[col] - gravity
            # ì¤‘ë ¥ ë²¡í„° í¬ê¸°ë„ ì €ìž¥
            df[f'{col}_gravity'] = gravity

        return df

    def extract_window_features(self, window_df):
        """ìœˆë„ìš°ë³„ íŠ¹ì§• ì¶”ì¶œ (ìµœì†Œ êµ¬ì„±)"""
        acc_cols = ['ax', 'ay', 'az']
        gyro_cols = ['gx', 'gy', 'gz']

        features = {}

        # ê°€ì†ë„ ë²¡í„° í¬ê¸°
        acc_mag = np.sqrt(window_df[acc_cols[0]]**2 +
                         window_df[acc_cols[1]]**2 +
                         window_df[acc_cols[2]]**2)

        # ìžì´ë¡œ ë²¡í„° í¬ê¸°
        gyro_mag = np.sqrt(window_df[gyro_cols[0]]**2 +
                          window_df[gyro_cols[1]]**2 +
                          window_df[gyro_cols[2]]**2)

        # === ì‹œê°„ì˜ì—­ íŠ¹ì„± ===
        features.update({
            'mean_acc': np.mean(acc_mag),
            'var_acc': np.var(acc_mag),
            'rms': np.sqrt(np.mean(acc_mag**2)),
            'sma': np.mean(np.abs(acc_mag)),
            'mean_gyro': np.mean(gyro_mag),
            'var_gyro': np.var(gyro_mag),
        })

        # Jerk (í‰í™œ í›„ 1ì°¨ ì°¨ë¶„)
        acc_smooth = signal.savgol_filter(acc_mag, 5, 2)
        jerk = np.sum(np.abs(np.diff(acc_smooth)))
        features['jerk'] = jerk

        # Peak count (prominence ê¸°ì¤€)
        peaks, properties = signal.find_peaks(acc_mag, prominence=0.5)
        features['peak_count'] = len(peaks)

        # Zero crossing rate
        zcr = np.sum(np.diff(np.sign(acc_mag - np.mean(acc_mag))) != 0)
        features['zcr'] = zcr

        # === ì£¼íŒŒìˆ˜ì˜ì—­ íŠ¹ì„± ===
        freqs, psd = signal.welch(acc_mag, fs=self.sampling_rate, nperseg=min(64, len(acc_mag)))

        # 0.5-5Hz ë°´ë“œíŒŒì›Œ
        band_mask = (freqs >= 0.5) & (freqs <= 5.0)
        band_power = np.sum(psd[band_mask])
        features['band_power'] = band_power

        # ì§€ë°° ì£¼íŒŒìˆ˜
        dom_freq = freqs[np.argmax(psd)]
        features['dom_freq'] = dom_freq

        # ìŠ¤íŽ™íŠ¸ëŸ´ ì—”íŠ¸ë¡œí”¼
        psd_norm = psd / np.sum(psd)
        spectral_entropy = -np.sum(psd_norm * np.log2(psd_norm + 1e-12))
        features['spectral_entropy'] = spectral_entropy

        # === ìžì„¸/ê²½ì‚¬ í”„ë¡ì‹œ ===
        if all(f'{col}_gravity' in window_df.columns for col in acc_cols):
            # ì¤‘ë ¥ë²¡í„°ì™€ì˜ ê°ë„
            gravity_vec = np.column_stack([window_df[f'{col}_gravity'] for col in acc_cols])
            gravity_angles = np.arccos(np.clip(gravity_vec[:, 2] /
                                             np.linalg.norm(gravity_vec, axis=1), -1, 1))
            features['gravity_angle_mean'] = np.mean(gravity_angles)
            features['gravity_angle_var'] = np.var(gravity_angles)

        return features

    def create_sliding_windows(self, df):
        """ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìƒì„±"""
        windows = []
        window_positions = []

        for start in range(0, len(df) - self.window_size + 1, self.stride):
            end = start + self.window_size
            window_df = df.iloc[start:end].copy()

            # ìœˆë„ìš° ë‚´ ëˆ„ë½ê°’ì´ ìžˆìœ¼ë©´ ìŠ¤í‚µ
            if window_df.isnull().any().any():
                continue

            windows.append(window_df)
            window_positions.append((start, end))

        return windows, window_positions

    def offline_fit(self, har_data_path=None, stair_data_path="data/raw/combined_dataset_stairclimbing.csv"):
        """ì˜¤í”„ë¼ì¸ í•™ìŠµ (HAR-PMD + Stairclimbing)"""
        print("ðŸŽ¯ ì˜¤í”„ë¼ì¸ ëª¨ë¸ í•™ìŠµ ì‹œìž‘")
        print("="*50)

        # === 1ë‹¨ê³„: Stairclimbing ë°ì´í„°ë¡œ ì‹œë“œ ë¼ë²¨ ìƒì„± ===
        print("\n1ï¸âƒ£ Stairclimbing ë°ì´í„° ì²˜ë¦¬...")
        stair_df = pd.read_csv(stair_data_path).dropna()

        # ì „ì²˜ë¦¬
        stair_df = self.preprocess_imu_stream(stair_df)

        # ìœˆë„ìš° ìƒì„±
        stair_windows, _ = self.create_sliding_windows(stair_df)
        print(f"   ìƒì„±ëœ ìœˆë„ìš°: {len(stair_windows)}ê°œ")

        # íŠ¹ì§• ì¶”ì¶œ
        stair_features = []
        stair_labels = []

        for window_df in stair_windows:
            features = self.extract_window_features(window_df)
            label = window_df['label'].mode()[0]  # ìœˆë„ìš° ë‚´ ìµœë¹ˆ ë¼ë²¨

            stair_features.append(list(features.values()))
            stair_labels.append(label)

        X_stair = np.array(stair_features)

        print(f"   ì¶”ì¶œëœ íŠ¹ì§•: {X_stair.shape}")
        print(f"   ë¼ë²¨ ë¶„í¬: {pd.Series(stair_labels).value_counts().to_dict()}")

        # === 2ë‹¨ê³„: íŠ¹ì§• ì „ì²˜ë¦¬ ===
        print("\n2ï¸âƒ£ íŠ¹ì§• ì „ì²˜ë¦¬...")
        X_stair_scaled = self.scaler.fit_transform(X_stair)

        # PCA (ì˜µì…˜)
        self.pca = PCA(n_components=0.95)  # 95% ë¶„ì‚° ë³´ì¡´
        X_stair_pca = self.pca.fit_transform(X_stair_scaled)

        print(f"   PCA ì°¨ì›: {X_stair.shape[1]} â†’ {X_stair_pca.shape[1]}")

        # === 3ë‹¨ê³„: ë‚œì´ë„ ì‹œë“œ ë¼ë²¨ ë§¤í•‘ ===
        print("\n3ï¸âƒ£ ë‚œì´ë„ ì‹œë“œ ë¼ë²¨ ë§¤í•‘...")
        difficulty_labels = self._map_to_difficulty(stair_labels)

        # === 4ë‹¨ê³„: ê¸°ì´ˆ ëª¨ë¸ í•™ìŠµ ===
        print("\n4ï¸âƒ£ ê¸°ì´ˆ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ...")
        self.base_model = RandomForestClassifier(
            n_estimators=100,
            random_state=42,
            class_weight='balanced'
        )

        self.base_model.fit(X_stair_pca, difficulty_labels)

        # êµì°¨ê²€ì¦ í‰ê°€
        cv_scores = []
        cv = LeaveOneGroupOut()
        groups = [label.split('_')[0] for label in stair_labels]  # í™œë™ ìœ í˜•ë³„ ê·¸ë£¹

        for train_idx, test_idx in cv.split(X_stair_pca, difficulty_labels, groups):
            X_train, X_test = X_stair_pca[train_idx], X_stair_pca[test_idx]
            y_train, y_test = np.array(difficulty_labels)[train_idx], np.array(difficulty_labels)[test_idx]

            temp_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
            temp_model.fit(X_train, y_train)

            y_pred = temp_model.predict(X_test)
            score = f1_score(y_test, y_pred, average='weighted')
            cv_scores.append(score)

        cv_mean = np.mean(cv_scores)
        cv_std = np.std(cv_scores)

        print(f"   êµì°¨ê²€ì¦ F1 ì ìˆ˜: {cv_mean:.3f} Â± {cv_std:.3f}")

        # === 5ë‹¨ê³„: ì•„í‹°íŒ©íŠ¸ ì €ìž¥ ===
        print("\n5ï¸âƒ£ ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ì €ìž¥...")
        self._save_artifacts()

        print(f"\nâœ… ì˜¤í”„ë¼ì¸ í•™ìŠµ ì™„ë£Œ!")
        return cv_mean

    def _map_to_difficulty(self, activity_labels):
        """í™œë™ ë¼ë²¨ì„ ë‚œì´ë„ë¡œ ë§¤í•‘"""
        activity_to_difficulty = {
            'up_stairs': 2,       # ê³„ë‹¨ ì˜¬ë¼ê°€ê¸°
            'down_stairs': 2,     # ê³„ë‹¨ ë‚´ë ¤ê°€ê¸°
            'up_largestairs': 3,  # í° ê³„ë‹¨ ì˜¬ë¼ê°€ê¸°
            'down_largestair': 3, # í° ê³„ë‹¨ ë‚´ë ¤ê°€ê¸°
            'up_slope': 1,        # ê²½ì‚¬ ì˜¬ë¼ê°€ê¸°
            'down_slope': 1,      # ê²½ì‚¬ ë‚´ë ¤ê°€ê¸°
        }

        return [activity_to_difficulty.get(label, 0) for label in activity_labels]

    def online_infer(self, stream_df):
        """ì˜¨ë¼ì¸ ì¶”ë¡  (ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼)"""
        print("ðŸ”® ì˜¨ë¼ì¸ ë‚œì´ë„ ì¶”ë¡ ...")

        # ì „ì²˜ë¦¬
        stream_df = self.preprocess_imu_stream(stream_df)

        # ìœˆë„ìš° ìƒì„±
        windows, positions = self.create_sliding_windows(stream_df)

        if not windows:
            print("âŒ ìœ íš¨í•œ ìœˆë„ìš°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None

        # íŠ¹ì§• ì¶”ì¶œ
        features_list = []
        for window_df in windows:
            features = self.extract_window_features(window_df)
            features_list.append(list(features.values()))

        X = np.array(features_list)

        # ì „ì²˜ë¦¬ ë³€í™˜
        X_scaled = self.scaler.transform(X)

        # PCAê°€ ì—†ìœ¼ë©´ ìŠ¤ì¼€ì¼ëœ ë°ì´í„° ê·¸ëŒ€ë¡œ ì‚¬ìš©
        if self.pca is not None:
            X_processed = self.pca.transform(X_scaled)
        else:
            X_processed = X_scaled

        # ë‚œì´ë„ ì˜ˆì¸¡
        difficulty_preds = self.base_model.predict(X_processed)
        difficulty_proba = self.base_model.predict_proba(X_processed)

        # ì—°ì†ì„± ë³´ì • (ìŠ¤ë¬´ë”©)
        difficulty_smoothed = self._smooth_predictions(difficulty_preds)

        # ì—£ì§€ ë¹„ìš© ê³„ì‚°
        edge_costs = self._calculate_edge_costs(difficulty_smoothed, positions)

        results = {
            'positions': positions,
            'difficulty_raw': difficulty_preds,
            'difficulty_smoothed': difficulty_smoothed,
            'probabilities': difficulty_proba,
            'edge_costs': edge_costs
        }

        print(f"âœ… {len(windows)}ê°œ ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ ì™„ë£Œ")
        return results

    def _smooth_predictions(self, predictions, min_duration=3):
        """ì„¸ê·¸ë¨¼íŠ¸ ìŠ¤ë¬´ë”© (ìµœì†Œ ì§€ì†ì‹œê°„ ì ìš©)"""
        if len(predictions) < min_duration:
            return predictions

        # ì–‘ë°©í–¥ ì´ë™í‰ê· 
        smoothed = signal.medfilt(predictions, kernel_size=min(min_duration, len(predictions)))

        return smoothed.astype(int)

    def _calculate_edge_costs(self, difficulties, positions):
        """ë‚œì´ë„ë¥¼ ì—£ì§€ ë¹„ìš©ìœ¼ë¡œ ë³€í™˜"""
        costs = []

        for i in range(len(difficulties)):
            # ê¸°ë³¸ ë¹„ìš© ë§¤í•‘
            difficulty_cost = difficulties[i] * 2  # 0,2,4,6

            # ê±°ë¦¬ ì„±ë¶„ (ìœˆë„ìš° ê¸¸ì´ ê¸°ì¤€)
            if i < len(positions):
                start, end = positions[i]
                distance_cost = self.alpha * (end - start)
            else:
                distance_cost = self.alpha * self.stride

            # ì´ ë¹„ìš©
            total_cost = distance_cost + self.beta * difficulty_cost
            costs.append(total_cost)

        return costs

    def _save_artifacts(self):
        """ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ì €ìž¥"""
        os.makedirs('models', exist_ok=True)

        artifacts = {
            'scaler': self.scaler,
            'pca': self.pca,
            'base_model': self.base_model,
            'difficulty_map': self.difficulty_map,
            'sampling_rate': self.sampling_rate,
            'window_size': self.window_size,
            'stride': self.stride,
            'alpha': self.alpha,
            'beta': self.beta,
            'gamma': self.gamma
        }

        with open('models/production_pipeline.pkl', 'wb') as f:
            pickle.dump(artifacts, f)

        print("âœ… ì•„í‹°íŒ©íŠ¸ ì €ìž¥: models/production_pipeline.pkl")

    def load_artifacts(self, path='models/production_pipeline.pkl'):
        """ì €ìž¥ëœ ì•„í‹°íŒ©íŠ¸ ë¡œë“œ"""
        with open(path, 'rb') as f:
            artifacts = pickle.load(f)

        self.scaler = artifacts['scaler']
        self.pca = artifacts['pca']
        self.base_model = artifacts['base_model']
        self.difficulty_map = artifacts['difficulty_map']

        print("âœ… ì•„í‹°íŒ©íŠ¸ ë¡œë“œ ì™„ë£Œ")

def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("ðŸš€ ì‹¤ì „ìš© IMU íŒŒì´í”„ë¼ì¸")
    print("="*50)

    # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    pipeline = ProductionIMUPipeline()

    # ì˜¤í”„ë¼ì¸ í•™ìŠµ
    cv_score = pipeline.offline_fit()

    print(f"\nðŸŽ‰ íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì™„ë£Œ!")
    print(f"ðŸ“Š ëª¨ë¸ ì„±ëŠ¥: F1={cv_score:.3f}")

if __name__ == "__main__":
    main()