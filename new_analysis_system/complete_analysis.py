#!/usr/bin/env python3
"""
ì™„ì „í•œ ë¶„ì„ íŒŒì´í”„ë¼ì¸ v2.0
ë°ì´í„° ë¡œë”© â†’ íŠ¹ì„± ì¶”ì¶œ â†’ í´ëŸ¬ìŠ¤í„°ë§ â†’ ë‚œì´ë„ í‰ê°€ â†’ íœ ì²´ì–´ ì ‘ê·¼ì„± ë¶„ì„ â†’ ì¢…í•© ë³´ê³ ì„œ
"""

import os
import sys
import pandas as pd
import numpy as np
from utils.data_loader import load_sensor_data, combine_sensor_data, get_data_info
from feature_extractor import FeatureExtractor
from clustering_analyzer import ClusteringAnalyzer
from difficulty_analyzer import DifficultyAnalyzer

def main():
    """ì™„ì „í•œ ë¶„ì„ íŒŒì´í”„ë¼ì¸ v2.0"""
    print("ğŸŒŠ Grid Island - ì™„ì „í•œ íœ ì²´ì–´ ì ‘ê·¼ì„± ë¶„ì„ ì‹œìŠ¤í…œ v2.0")
    print("ğŸ¯ íŠ¹ì„± ì¶”ì¶œ â†’ í´ëŸ¬ìŠ¤í„°ë§ â†’ ë‚œì´ë„ í‰ê°€ â†’ íœ ì²´ì–´ ì ‘ê·¼ì„± ë¶„ì„ â†’ ì¢…í•© ë³´ê³ ì„œ")
    print("=" * 75)

    # ê²°ê³¼ í´ë” ìƒì„±
    os.makedirs("results", exist_ok=True)

    # 1ë‹¨ê³„: ì„¼ì„œ ë°ì´í„° ì²˜ë¦¬
    print("\nğŸ“Š 1ë‹¨ê³„: ì„¼ì„œ ë°ì´í„° ì²˜ë¦¬")
    print("-" * 45)

    data_dir = "data/test 2025-09-22 18-30-21"
    sensor_data = load_sensor_data(data_dir)

    if not sensor_data:
        print("âŒ ì„¼ì„œ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨")
        return False

    combined_df = combine_sensor_data(sensor_data)
    data_info = get_data_info(combined_df)

    print(f"ğŸ“ˆ ë°ì´í„° í’ˆì§ˆ í‰ê°€:")
    print(f"  ìƒ˜í”Œë§ í’ˆì§ˆ: {'ìš°ìˆ˜' if data_info['sampling_rate_hz'] > 45 else 'ë³´í†µ'}")
    print(f"  ë°ì´í„° ì™„ì„±ë„: {(len(combined_df)/data_info['total_samples']*100):.1f}%")

    # 2ë‹¨ê³„: ê³ ê¸‰ íŠ¹ì„± ì¶”ì¶œ
    print("\nğŸ” 2ë‹¨ê³„: 55ì°¨ì› íŠ¹ì„± ì¶”ì¶œ ë° ì—”ì§€ë‹ˆì–´ë§")
    print("-" * 50)

    extractor = FeatureExtractor(window_size=200, overlap_ratio=0.75)
    features_df, window_positions = extractor.process_data(combined_df)

    # íŠ¹ì„± í’ˆì§ˆ í‰ê°€
    feature_quality = {
        'completeness': features_df.isnull().sum().sum() / (len(features_df) * len(features_df.columns)),
        'variance_ratio': len(features_df.columns[features_df.var() > 0.001]) / len(features_df.columns)
    }

    print(f"ğŸ“Š íŠ¹ì„± í’ˆì§ˆ í‰ê°€:")
    print(f"  íŠ¹ì„± ì™„ì„±ë„: {(1-feature_quality['completeness'])*100:.1f}%")
    print(f"  ìœ ì˜ë¯¸ íŠ¹ì„± ë¹„ìœ¨: {feature_quality['variance_ratio']*100:.1f}%")

    features_path = "results/extracted_features.csv"
    features_df.to_csv(features_path, index=False)

    # 3ë‹¨ê³„: í´ëŸ¬ìŠ¤í„°ë§
    print("\nğŸ¯ 3ë‹¨ê³„: í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„")
    print("-" * 40)

    analyzer = ClusteringAnalyzer()
    analyzer.load_features(features_path)
    features_scaled = analyzer.preprocess_features()
    features_pca = analyzer.apply_pca(n_components=10)

    # ë‹¤ì¤‘ ë°©ë²•ë¡  ì ìš©
    methods = ['kmeans', 'gmm']
    best_result = None
    best_score = -1

    for method in methods:
        optimization_results = analyzer.find_optimal_clusters(max_k=8, method=method)
        max_silhouette = max(optimization_results['silhouette'])
        if max_silhouette > best_score:
            best_score = max_silhouette
            best_result = (method, optimization_results)

    method, optimization_results = best_result
    optimal_k = optimization_results['k_values'][np.argmax(optimization_results['silhouette'])]

    print(f"ğŸ† ìµœì  ë°©ë²•ë¡ : {method.upper()}, K={optimal_k} (í’ˆì§ˆ: {best_score:.3f})")

    labels = analyzer.perform_clustering(n_clusters=optimal_k, method=method)

    # ê²°ê³¼ ì €ì¥
    result_df = features_df.copy()
    result_df['cluster'] = labels
    result_df.to_csv("results/clustered_features.csv", index=False)

    # ì‹œê°í™”
    try:
        import matplotlib
        matplotlib.use('Agg')
        analyzer.visualize_clusters(labels, save_path="results/clustering_visualization.png")
    except Exception as e:
        print(f"âš ï¸  ì‹œê°í™” ì €ì¥ ì‹¤íŒ¨: {e}")

    # ìµœì¢… ìš”ì•½ ì¶œë ¥
    print(f"\nğŸ‰ ë¶„ì„ ì™„ë£Œ!")
    print("=" * 60)

    return True

if __name__ == "__main__":
    success = main()
    if success:
        print(f"\nğŸŠ ë¶„ì„ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ! ìƒì„¸ ê²°ê³¼ëŠ” results/ í´ë”ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    else:
        print(f"\nğŸ’¥ ë¶„ì„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")