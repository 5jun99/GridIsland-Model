#!/usr/bin/env python3
"""
ì™„ì „í•œ ë¶„ì„ íŒŒì´í”„ë¼ì¸
ë°ì´í„° ë¡œë”© â†’ íŠ¹ì„± ì¶”ì¶œ â†’ í´ëŸ¬ìŠ¤í„°ë§ â†’ ë‚œì´ë„ í‰ê°€ â†’ íœ ì²´ì–´ ì ‘ê·¼ì„± ë¶„ì„
"""

import os
import sys
import pandas as pd
import numpy as np
from utils.data_loader import load_sensor_data, combine_sensor_data, get_data_info
from feature_extractor import FeatureExtractor
from clustering_analyzer import ClusteringAnalyzer
from difficulty_analyzer import DifficultyAnalyzer

def main():
    """ì™„ì „í•œ ë¶„ì„ íŒŒì´í”„ë¼ì¸"""
    print("ğŸŒŠ Grid Island - ì™„ì „í•œ Test ë°ì´í„° ë¶„ì„ ì‹œìŠ¤í…œ")
    print("ğŸ¯ íŠ¹ì„± ì¶”ì¶œ â†’ í´ëŸ¬ìŠ¤í„°ë§ â†’ ë‚œì´ë„ í‰ê°€ â†’ íœ ì²´ì–´ ì ‘ê·¼ì„± ë¶„ì„")
    print("=" * 70)

    # ê²°ê³¼ í´ë” ìƒì„±
    os.makedirs("results", exist_ok=True)

    # 1ë‹¨ê³„: ë°ì´í„° ë¡œë”©
    print("\nğŸ“Š 1ë‹¨ê³„: ì„¼ì„œ ë°ì´í„° ë¡œë”©")
    print("-" * 40)

    data_dir = "data/test 2025-09-22 18-30-21"
    sensor_data = load_sensor_data(data_dir)

    if not sensor_data:
        print("âŒ ì„¼ì„œ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨")
        return

    combined_df = combine_sensor_data(sensor_data)
    data_info = get_data_info(combined_df)

    print(f"ğŸ“‹ ë°ì´í„° ì •ë³´:")
    for key, value in data_info.items():
        if key != 'columns':
            print(f"  {key}: {value}")

    # 2ë‹¨ê³„: íŠ¹ì„± ì¶”ì¶œ
    print("\nğŸ” 2ë‹¨ê³„: íŠ¹ì„± ì¶”ì¶œ")
    print("-" * 40)

    extractor = FeatureExtractor(window_size=200, overlap_ratio=0.75)
    features_df, window_positions = extractor.process_data(combined_df)

    # íŠ¹ì„± ë°ì´í„° ì €ì¥
    features_path = "results/extracted_features.csv"
    features_df.to_csv(features_path, index=False)
    print(f"ğŸ’¾ íŠ¹ì„± ë°ì´í„° ì €ì¥: {features_path}")

    print(f"ğŸ“Š ì¶”ì¶œ ê²°ê³¼:")
    print(f"  ìœˆë„ìš° ìˆ˜: {len(features_df)}")
    print(f"  íŠ¹ì„± ìˆ˜: {len(features_df.columns)}")

    # 3ë‹¨ê³„: í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„
    print("\nğŸ¯ 3ë‹¨ê³„: í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„")
    print("-" * 40)

    analyzer = ClusteringAnalyzer()
    analyzer.load_features(features_path)

    # ì „ì²˜ë¦¬
    features_scaled = analyzer.preprocess_features()

    # PCA ì ìš©
    features_pca = analyzer.apply_pca(n_components=10)

    # ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ íƒìƒ‰
    print("\nğŸ” ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ íƒìƒ‰:")
    optimization_results = analyzer.find_optimal_clusters(max_k=8, method='kmeans')

    # ì¶”ì²œ í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì •
    silhouette_scores = optimization_results['silhouette']
    optimal_k = optimization_results['k_values'][np.argmax(silhouette_scores)]
    print(f"ğŸ“ˆ ì¶”ì²œ í´ëŸ¬ìŠ¤í„° ìˆ˜: {optimal_k} (Silhouette Score: {max(silhouette_scores):.3f})")

    # í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
    print(f"\nğŸ¯ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰ (K={optimal_k}):")
    labels = analyzer.perform_clustering(n_clusters=optimal_k, method='kmeans')

    # í´ëŸ¬ìŠ¤í„° íŠ¹ì„± ë¶„ì„
    cluster_characteristics = analyzer.analyze_cluster_characteristics(labels)

    # í´ëŸ¬ìŠ¤í„°ë§ ì‹œê°í™”
    try:
        import matplotlib
        matplotlib.use('Agg')
        analyzer.visualize_clusters(labels, save_path="results/clustering_visualization.png")
    except Exception as e:
        print(f"âš ï¸  í´ëŸ¬ìŠ¤í„°ë§ ì‹œê°í™” ì €ì¥ ì‹¤íŒ¨: {e}")

    # ê²°ê³¼ ì €ì¥
    result_df = features_df.copy()
    result_df['cluster'] = labels
    result_df.to_csv("results/clustered_features.csv", index=False)
    cluster_characteristics.to_csv("results/cluster_characteristics.csv", index=False)

    # 4ë‹¨ê³„: ë‚œì´ë„ ë° íœ ì²´ì–´ ì ‘ê·¼ì„± ë¶„ì„
    print("\nğŸ¯ 4ë‹¨ê³„: ë‚œì´ë„ ë° íœ ì²´ì–´ ì ‘ê·¼ì„± ë¶„ì„")
    print("-" * 50)

    difficulty_analyzer = DifficultyAnalyzer()
    difficulty_analyzer.load_cluster_data("results/cluster_characteristics.csv")

    # ë‚œì´ë„ ë¶„ì„ ìˆ˜í–‰
    difficulty_results = difficulty_analyzer.analyze_all_clusters()

    # ë‚œì´ë„ ë¶„ì„ ì‹œê°í™”
    try:
        difficulty_analyzer.visualize_analysis(difficulty_results, save_path="results/difficulty_analysis.png")
    except Exception as e:
        print(f"âš ï¸  ë‚œì´ë„ ë¶„ì„ ì‹œê°í™” ì €ì¥ ì‹¤íŒ¨: {e}")

    # ë³´ê³ ì„œ ìƒì„±
    report = difficulty_analyzer.generate_report(difficulty_results)

    # ê²°ê³¼ ì €ì¥
    difficulty_results.to_csv("results/difficulty_analysis.csv", index=False)
    with open("results/difficulty_report.txt", "w", encoding="utf-8") as f:
        f.write(report)

    # 5ë‹¨ê³„: ì¢…í•© ìš”ì•½
    print("\nğŸ“‹ 5ë‹¨ê³„: ì¢…í•© ë¶„ì„ ìš”ì•½")
    print("-" * 40)

    print(f"âœ… ë¶„ì„ ì™„ë£Œ!")
    print(f"\nğŸ“Š ë°ì´í„° ê·œëª¨:")
    print(f"  ì›ë³¸ ì„¼ì„œ ë°ì´í„°: {len(combined_df):,}ê°œ ìƒ˜í”Œ")
    print(f"  ì¸¡ì • ì‹œê°„: {data_info['duration_seconds']:.1f}ì´ˆ")
    print(f"  ìƒ˜í”Œë§ ì£¼íŒŒìˆ˜: {data_info['sampling_rate_hz']:.1f}Hz")

    print(f"\nğŸ” íŠ¹ì„± ì¶”ì¶œ ê²°ê³¼:")
    print(f"  ìœˆë„ìš° ìˆ˜: {len(features_df):,}ê°œ")
    print(f"  ì¶”ì¶œëœ íŠ¹ì„±: {len(features_df.columns)}ê°œ")

    print(f"\nğŸ¯ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼:")
    print(f"  ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜: {optimal_k}ê°œ")
    print(f"  Silhouette Score: {max(silhouette_scores):.3f}")

    print(f"\nğŸ¥ íœ ì²´ì–´ ì ‘ê·¼ì„± í‰ê°€:")
    for idx, row in difficulty_results.iterrows():
        status_emoji = "âœ…" if row.wheelchair_score >= 0.6 else "âš ï¸" if row.wheelchair_score >= 0.4 else "âŒ"
        print(f"  {status_emoji} í´ëŸ¬ìŠ¤í„° {row.cluster}: {row.wheelchair_grade}ë“±ê¸‰ ({row.wheelchair_score:.3f}) - {row.wheelchair_name}")

    # ì „ì²´ ê²½ë¡œ í‰ê°€
    weighted_accessibility = (difficulty_results['wheelchair_score'] * difficulty_results['percentage'] / 100).sum()
    overall_emoji = "âœ…" if weighted_accessibility >= 0.6 else "âš ï¸" if weighted_accessibility >= 0.4 else "âŒ"
    print(f"\nğŸ¯ ì „ì²´ ê²½ë¡œ í‰ê°€:")
    print(f"  {overall_emoji} ì¢…í•© íœ ì²´ì–´ ì ‘ê·¼ì„±: {weighted_accessibility:.3f}")

    if weighted_accessibility >= 0.6:
        recommendation = "íœ ì²´ì–´ ì´ìš©ì— ì í•©í•œ ê²½ë¡œì…ë‹ˆë‹¤"
    elif weighted_accessibility >= 0.4:
        recommendation = "íœ ì²´ì–´ ì´ìš© ì‹œ ì£¼ì˜ê°€ í•„ìš”í•œ ê²½ë¡œì…ë‹ˆë‹¤"
    else:
        recommendation = "íœ ì²´ì–´ ì´ìš©ì´ ì–´ë ¤ìš´ ê²½ë¡œì…ë‹ˆë‹¤"

    print(f"  ğŸ’¡ ê¶Œì¥ì‚¬í•­: {recommendation}")

    # í´ëŸ¬ìŠ¤í„°ë³„ ìƒì„¸ ì •ë³´ í‘œì‹œ
    print(f"\nğŸ“Š í´ëŸ¬ìŠ¤í„°ë³„ ìƒì„¸ ì •ë³´:")
    for idx, row in difficulty_results.iterrows():
        print(f"\n  ğŸ”¸ í´ëŸ¬ìŠ¤í„° {row.cluster} ({row.count}ê°œ ìœˆë„ìš°, {row.percentage:.1f}%):")
        print(f"     ë‚œì´ë„: {row.difficulty_name} ({row.difficulty_score:.3f})")
        print(f"     íœ ì²´ì–´ ì ‘ê·¼ì„±: {row.wheelchair_grade}ë“±ê¸‰ - {row.wheelchair_name}")
        print(f"     ì„¤ëª…: {row.difficulty_description}")

    print(f"\nğŸ“ ì €ì¥ëœ ê²°ê³¼ íŒŒì¼:")
    result_files = [
        "results/extracted_features.csv",
        "results/clustered_features.csv",
        "results/cluster_characteristics.csv",
        "results/difficulty_analysis.csv",
        "results/difficulty_report.txt",
        "results/clustering_visualization.png",
        "results/difficulty_analysis.png"
    ]

    for file in result_files:
        if os.path.exists(file):
            print(f"  âœ… {file}")
        else:
            print(f"  âŒ {file}")

    print(f"\nğŸ‰ Grid Island ì™„ì „ ë¶„ì„ ì™„ë£Œ!")
    print(f"ğŸ“– ìƒì„¸í•œ ë¶„ì„ ê²°ê³¼ëŠ” results/difficulty_report.txtë¥¼ í™•ì¸í•˜ì„¸ìš”")

    # ë¶„ì„ ê²°ê³¼ ì¶œë ¥
    print(f"\n{report}")

if __name__ == "__main__":
    main()