#!/usr/bin/env python3
"""
í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ê¸° - ì¶”ì¶œëœ íŠ¹ì„±ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans, DBSCAN
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.manifold import TSNE
import warnings
warnings.filterwarnings('ignore')

class ClusteringAnalyzer:
    """í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ê¸°"""

    def __init__(self):
        self.scaler = StandardScaler()
        self.pca = None
        self.features_df = None
        self.features_scaled = None
        self.features_pca = None

    def load_features(self, features_path: str) -> pd.DataFrame:
        """íŠ¹ì„± ë°ì´í„° ë¡œë“œ"""
        print(f"ğŸ“Š íŠ¹ì„± ë°ì´í„° ë¡œë“œ: {features_path}")
        self.features_df = pd.read_csv(features_path)
        print(f"âœ… ë¡œë“œ ì™„ë£Œ: {len(self.features_df)} ìœˆë„ìš°, {len(self.features_df.columns)} íŠ¹ì„±")
        return self.features_df

    def preprocess_features(self, exclude_cols: list = None) -> np.ndarray:
        """íŠ¹ì„± ì „ì²˜ë¦¬ (ìŠ¤ì¼€ì¼ë§, ì°¨ì›ì¶•ì†Œ)"""
        if self.features_df is None:
            raise ValueError("íŠ¹ì„± ë°ì´í„°ë¥¼ ë¨¼ì € ë¡œë“œí•˜ì„¸ìš”")

        # ì œì™¸í•  ì»¬ëŸ¼ë“¤ (ë©”íƒ€ë°ì´í„°)
        if exclude_cols is None:
            exclude_cols = ['window_id', 'start_idx', 'end_idx', 'window_size']

        # ìˆ«ìí˜• íŠ¹ì„±ë§Œ ì„ íƒ
        numeric_cols = self.features_df.select_dtypes(include=[np.number]).columns
        feature_cols = [col for col in numeric_cols if col not in exclude_cols]

        print(f"ğŸ”§ ì „ì²˜ë¦¬í•  íŠ¹ì„±: {len(feature_cols)}ê°œ")

        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        features_clean = self.features_df[feature_cols].fillna(0)

        # ë¬´í•œëŒ€ê°’ ì²˜ë¦¬
        features_clean = features_clean.replace([np.inf, -np.inf], 0)

        # í‘œì¤€í™”
        self.features_scaled = self.scaler.fit_transform(features_clean)
        print(f"âœ… í‘œì¤€í™” ì™„ë£Œ: {self.features_scaled.shape}")

        return self.features_scaled

    def apply_pca(self, n_components: int = 10) -> np.ndarray:
        """PCA ì°¨ì› ì¶•ì†Œ"""
        if self.features_scaled is None:
            raise ValueError("íŠ¹ì„±ì„ ë¨¼ì € ì „ì²˜ë¦¬í•˜ì„¸ìš”")

        self.pca = PCA(n_components=n_components)
        self.features_pca = self.pca.fit_transform(self.features_scaled)

        # ì„¤ëª… ë¶„ì‚° ì¶œë ¥
        explained_var = self.pca.explained_variance_ratio_
        cumulative_var = np.cumsum(explained_var)

        print(f"ğŸ“Š PCA ê²°ê³¼ ({n_components}ê°œ ì»´í¬ë„ŒíŠ¸):")
        print(f"  ì´ ì„¤ëª… ë¶„ì‚°: {cumulative_var[-1]:.3f}")
        print(f"  ê° ì»´í¬ë„ŒíŠ¸ ê¸°ì—¬ë„: {explained_var[:5]}")

        return self.features_pca

    def find_optimal_clusters(self, max_k: int = 10, method: str = 'kmeans') -> dict:
        """ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ì°¾ê¸°"""
        if self.features_scaled is None:
            raise ValueError("íŠ¹ì„±ì„ ë¨¼ì € ì „ì²˜ë¦¬í•˜ì„¸ìš”")

        print(f"ğŸ” ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ íƒìƒ‰ (K=2~{max_k}, ë°©ë²•: {method})")

        results = {
            'k_values': [],
            'inertia': [],
            'silhouette': [],
            'calinski_harabasz': [],
            'davies_bouldin': []
        }

        for k in range(2, max_k + 1):
            if method == 'kmeans':
                clusterer = KMeans(n_clusters=k, random_state=42, n_init=10)
            elif method == 'gmm':
                clusterer = GaussianMixture(n_components=k, random_state=42)
            else:
                raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°©ë²•ì…ë‹ˆë‹¤")

            labels = clusterer.fit_predict(self.features_scaled)

            # í‰ê°€ ì§€í‘œ ê³„ì‚°
            silhouette = silhouette_score(self.features_scaled, labels)
            calinski = calinski_harabasz_score(self.features_scaled, labels)
            davies_bouldin = davies_bouldin_score(self.features_scaled, labels)

            results['k_values'].append(k)
            results['silhouette'].append(silhouette)
            results['calinski_harabasz'].append(calinski)
            results['davies_bouldin'].append(davies_bouldin)

            if method == 'kmeans':
                results['inertia'].append(clusterer.inertia_)

            print(f"  K={k}: Silhouette={silhouette:.3f}, Calinski={calinski:.1f}, Davies-Bouldin={davies_bouldin:.3f}")

        return results

    def perform_clustering(self, n_clusters: int = 5, method: str = 'kmeans') -> np.ndarray:
        """í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰"""
        if self.features_scaled is None:
            raise ValueError("íŠ¹ì„±ì„ ë¨¼ì € ì „ì²˜ë¦¬í•˜ì„¸ìš”")

        print(f"ğŸ¯ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰: {method}, K={n_clusters}")

        if method == 'kmeans':
            clusterer = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        elif method == 'gmm':
            clusterer = GaussianMixture(n_components=n_clusters, random_state=42)
        elif method == 'dbscan':
            clusterer = DBSCAN(eps=0.5, min_samples=5)
        else:
            raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°©ë²•ì…ë‹ˆë‹¤")

        labels = clusterer.fit_predict(self.features_scaled)

        # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ í‰ê°€
        if len(set(labels)) > 1:  # í´ëŸ¬ìŠ¤í„°ê°€ 2ê°œ ì´ìƒì¸ ê²½ìš°ë§Œ
            silhouette = silhouette_score(self.features_scaled, labels)
            calinski = calinski_harabasz_score(self.features_scaled, labels)
            davies_bouldin = davies_bouldin_score(self.features_scaled, labels)

            print(f"ğŸ“Š í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼:")
            print(f"  í´ëŸ¬ìŠ¤í„° ìˆ˜: {len(set(labels))}")
            print(f"  Silhouette Score: {silhouette:.3f}")
            print(f"  Calinski-Harabasz Index: {calinski:.1f}")
            print(f"  Davies-Bouldin Index: {davies_bouldin:.3f}")

            # í´ëŸ¬ìŠ¤í„°ë³„ ë¶„í¬
            unique, counts = np.unique(labels, return_counts=True)
            print(f"  í´ëŸ¬ìŠ¤í„° ë¶„í¬:")
            for cluster, count in zip(unique, counts):
                print(f"    í´ëŸ¬ìŠ¤í„° {cluster}: {count}ê°œ ({count/len(labels)*100:.1f}%)")

        return labels

    def visualize_clusters(self, labels: np.ndarray, save_path: str = None):
        """í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì‹œê°í™”"""
        if self.features_scaled is None:
            raise ValueError("íŠ¹ì„±ì„ ë¨¼ì € ì „ì²˜ë¦¬í•˜ì„¸ìš”")

        # t-SNEë¡œ 2D ì‹œê°í™”
        print("ğŸ“Š t-SNE ì‹œê°í™” ìƒì„± ì¤‘...")
        tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(labels)-1))
        features_2d = tsne.fit_transform(self.features_scaled)

        # í”Œë¡¯ ìƒì„±
        plt.figure(figsize=(12, 8))

        # ì„œë¸Œí”Œë¡¯ 1: t-SNE ê²°ê³¼
        plt.subplot(2, 2, 1)
        scatter = plt.scatter(features_2d[:, 0], features_2d[:, 1], c=labels, cmap='tab10', alpha=0.7)
        plt.title('t-SNE Visualization of Clusters')
        plt.xlabel('t-SNE 1')
        plt.ylabel('t-SNE 2')
        plt.colorbar(scatter)

        # ì„œë¸Œí”Œë¡¯ 2: PCAê°€ ìˆìœ¼ë©´ PCA ê²°ê³¼
        if self.features_pca is not None:
            plt.subplot(2, 2, 2)
            scatter = plt.scatter(self.features_pca[:, 0], self.features_pca[:, 1], c=labels, cmap='tab10', alpha=0.7)
            plt.title('PCA Visualization of Clusters')
            plt.xlabel('PC1')
            plt.ylabel('PC2')
            plt.colorbar(scatter)

        # ì„œë¸Œí”Œë¡¯ 3: í´ëŸ¬ìŠ¤í„° ë¶„í¬
        plt.subplot(2, 2, 3)
        unique, counts = np.unique(labels, return_counts=True)
        plt.bar(unique, counts, alpha=0.7)
        plt.title('Cluster Distribution')
        plt.xlabel('Cluster')
        plt.ylabel('Count')

        # ì„œë¸Œí”Œë¡¯ 4: ì‹œê°„ì— ë”°ë¥¸ í´ëŸ¬ìŠ¤í„° ë³€í™”
        plt.subplot(2, 2, 4)
        plt.plot(labels, alpha=0.7)
        plt.title('Clusters over Time')
        plt.xlabel('Window Index')
        plt.ylabel('Cluster')

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ’¾ ì‹œê°í™” ì €ì¥: {save_path}")

        plt.show()

    def analyze_cluster_characteristics(self, labels: np.ndarray) -> pd.DataFrame:
        """í´ëŸ¬ìŠ¤í„°ë³„ íŠ¹ì„± ë¶„ì„"""
        if self.features_df is None:
            raise ValueError("íŠ¹ì„± ë°ì´í„°ë¥¼ ë¨¼ì € ë¡œë“œí•˜ì„¸ìš”")

        # í´ëŸ¬ìŠ¤í„° ë¼ë²¨ ì¶”ê°€
        analysis_df = self.features_df.copy()
        analysis_df['cluster'] = labels

        # ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ ì„ íƒ
        numeric_cols = analysis_df.select_dtypes(include=[np.number]).columns
        exclude_cols = ['window_id', 'start_idx', 'end_idx', 'window_size', 'cluster']
        feature_cols = [col for col in numeric_cols if col not in exclude_cols]

        # í´ëŸ¬ìŠ¤í„°ë³„ í†µê³„
        cluster_stats = []

        for cluster in sorted(analysis_df['cluster'].unique()):
            cluster_data = analysis_df[analysis_df['cluster'] == cluster]
            stats = {
                'cluster': cluster,
                'count': len(cluster_data),
                'percentage': len(cluster_data) / len(analysis_df) * 100
            }

            # ì£¼ìš” íŠ¹ì„±ë“¤ì˜ í‰ê· ê°’
            for col in feature_cols[:20]:  # ìƒìœ„ 20ê°œ íŠ¹ì„±
                stats[f'{col}_mean'] = cluster_data[col].mean()

            cluster_stats.append(stats)

        cluster_stats_df = pd.DataFrame(cluster_stats)
        return cluster_stats_df

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¯ í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ì‹œì‘")
    print("=" * 60)

    # ë¶„ì„ê¸° ìƒì„±
    analyzer = ClusteringAnalyzer()

    # íŠ¹ì„± ë°ì´í„° ë¡œë“œ
    features_df = analyzer.load_features("results/extracted_features.csv")

    # ì „ì²˜ë¦¬
    features_scaled = analyzer.preprocess_features()

    # PCA ì ìš©
    features_pca = analyzer.apply_pca(n_components=10)

    # ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ì°¾ê¸°
    optimization_results = analyzer.find_optimal_clusters(max_k=8, method='kmeans')

    # í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰ (K=5ë¡œ ì„¤ì •)
    labels = analyzer.perform_clustering(n_clusters=5, method='kmeans')

    # ê²°ê³¼ ì‹œê°í™”
    analyzer.visualize_clusters(labels, save_path="results/clustering_visualization.png")

    # í´ëŸ¬ìŠ¤í„° íŠ¹ì„± ë¶„ì„
    cluster_characteristics = analyzer.analyze_cluster_characteristics(labels)

    # ê²°ê³¼ ì €ì¥
    import os
    os.makedirs("results", exist_ok=True)

    # ë¼ë²¨ì´ í¬í•¨ëœ ë°ì´í„° ì €ì¥
    result_df = features_df.copy()
    result_df['cluster'] = labels
    result_df.to_csv("results/clustered_features.csv", index=False)

    # í´ëŸ¬ìŠ¤í„° íŠ¹ì„± ì €ì¥
    cluster_characteristics.to_csv("results/cluster_characteristics.csv", index=False)

    print(f"\nâœ… ë¶„ì„ ì™„ë£Œ!")
    print(f"ğŸ“Š ê²°ê³¼ íŒŒì¼:")
    print(f"  - results/clustered_features.csv")
    print(f"  - results/cluster_characteristics.csv")
    print(f"  - results/clustering_visualization.png")

if __name__ == "__main__":
    main()