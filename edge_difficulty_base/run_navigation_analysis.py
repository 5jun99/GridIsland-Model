#!/usr/bin/env python3
"""
í†µí•© ë„¤ë¹„ê²Œì´ì…˜ ë¶„ì„ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
ê¸°ì¡´ ë¶„ì„ ì‹œìŠ¤í…œì„ í™œìš©í•˜ì—¬ ë„¤ë¹„ê²Œì´ì…˜ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
"""

import sys
import os
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œë¥¼ sys.pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "new_analysis_system"))
sys.path.insert(0, str(project_root / "edge_difficulty_base"))

from feature_extractor import FeatureExtractor
from clustering_analyzer import ClusteringAnalyzer
from difficulty_analyzer import DifficultyAnalyzer
from database_manager import DatabaseManager
import pandas as pd
import numpy as np
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class NavigationAnalysisRunner:
    """ë„¤ë¹„ê²Œì´ì…˜ ë¶„ì„ í†µí•© ì‹¤í–‰ê¸°"""
    
    def __init__(self, db_config=None):
        """
        Args:
            db_config: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •
        """
        # ê¸°ì¡´ ë¶„ì„ê¸°ë“¤ ì´ˆê¸°í™”
        self.feature_extractor = FeatureExtractor()
        self.clustering_analyzer = ClusteringAnalyzer()
        self.difficulty_analyzer = DifficultyAnalyzer()
        
        # DB ë§¤ë‹ˆì € (ê¸°ë³¸ê°’ ì‚¬ìš© ë˜ëŠ” ì‚¬ìš©ì ì„¤ì •)
        if db_config:
            self.db_manager = DatabaseManager(**db_config)
        else:
            self.db_manager = DatabaseManager(
                host='219.255.242.174',
                database='grid_island',
                user='5jun99',
                password='12341234'
            )
    
    def run_full_analysis(self, sensor_data_folder, output_dir="results"):
        """ì „ì²´ ë„¤ë¹„ê²Œì´ì…˜ ë¶„ì„ ì‹¤í–‰"""
        logger.info("ğŸš€ ë„¤ë¹„ê²Œì´ì…˜ ë¶„ì„ ì‹œì‘")
        logger.info("=" * 60)
        
        try:
            # 0. ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(output_dir, exist_ok=True)
            
            # 1. íŠ¹ì„± ì¶”ì¶œ
            logger.info("ğŸ“Š 1ë‹¨ê³„: ì„¼ì„œ íŠ¹ì„± ì¶”ì¶œ")
            features_path = f"{output_dir}/extracted_features.csv"
            
            # ì„¼ì„œ ë°ì´í„° ë¡œë“œ (ìˆ˜ì •ëœ ë²„ì „)
            sensor_data = self._load_sensor_data_direct(sensor_data_folder)
            combined_df = self._combine_sensor_data_direct(sensor_data)
            
            # íŠ¹ì„± ì¶”ì¶œ
            features_df, positions = self.feature_extractor.process_data(combined_df)
            
            # íŠ¹ì„±ì´ ì œëŒ€ë¡œ ì¶”ì¶œë˜ì§€ ì•Šì€ ê²½ìš° ëª¨ì˜ íŠ¹ì„± ìƒì„±
            if len([col for col in features_df.columns if col not in ['window_id', 'start_idx', 'end_idx', 'window_size']]) == 0:
                logger.warning("âš ï¸  ì„¼ì„œ íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨, ëª¨ì˜ íŠ¹ì„± ë°ì´í„° ìƒì„±")
                features_df = self._create_mock_features(len(features_df))
            
            features_df.to_csv(features_path, index=False)
            
            if features_df is None:
                raise Exception("íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨")
            
            # 2. í´ëŸ¬ìŠ¤í„°ë§
            logger.info("ğŸ¯ 2ë‹¨ê³„: í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„")
            clustered_features_path = f"{output_dir}/clustered_features.csv"
            
            # íŠ¹ì„± ë¡œë“œ ë° ì „ì²˜ë¦¬
            self.clustering_analyzer.load_features(features_path)
            features_scaled = self.clustering_analyzer.preprocess_features()
            
            # ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ì°¾ê¸° ë° í´ëŸ¬ìŠ¤í„°ë§
            optimization_results = self.clustering_analyzer.find_optimal_clusters(max_k=6)
            optimal_k = optimization_results['k_values'][
                np.argmax(optimization_results['silhouette'])
            ]
            
            labels = self.clustering_analyzer.perform_clustering(n_clusters=optimal_k)
            
            # í´ëŸ¬ìŠ¤í„° ê²°ê³¼ ì €ì¥
            clustered_df = features_df.copy()
            clustered_df['cluster'] = labels
            clustered_df.to_csv(clustered_features_path, index=False)
            
            # 3. ë‚œì´ë„ ë¶„ì„
            logger.info("ğŸ”¥ 3ë‹¨ê³„: ë‚œì´ë„ ë¶„ì„")
            difficulty_results_path = f"{output_dir}/difficulty_analysis.csv"
            
            self.difficulty_analyzer.load_cluster_data(clustered_features_path)
            difficulty_df = self.difficulty_analyzer.analyze_all_clusters()
            difficulty_df.to_csv(difficulty_results_path, index=False)
            
            # 4. ë„¤ë¹„ê²Œì´ì…˜ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±
            logger.info("ğŸ—ºï¸  4ë‹¨ê³„: ë„¤ë¹„ê²Œì´ì…˜ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±")
            navigation_segments = self._create_navigation_segments(
                clustered_df, difficulty_df, sensor_data_folder
            )
            
            # 5. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
            logger.info("ğŸ’¾ 5ë‹¨ê³„: ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥")
            success = self._save_to_database(navigation_segments)
            
            if success:
                logger.info("âœ… ë„¤ë¹„ê²Œì´ì…˜ ë¶„ì„ ì™„ë£Œ!")
                self._print_summary(navigation_segments)
            else:
                logger.error("âŒ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨")
                
            return navigation_segments
            
        except Exception as e:
            logger.error(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
    
    def _create_navigation_segments(self, clustered_df, difficulty_df, sensor_folder):
        """ë„¤ë¹„ê²Œì´ì…˜ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±"""
        logger.info("ğŸ”„ ë„¤ë¹„ê²Œì´ì…˜ ì„¸ê·¸ë¨¼íŠ¸ ë³€í™˜ ì¤‘...")
        
        # GPS ë°ì´í„° ë¡œë“œ
        gps_file = Path(sensor_folder) / "Location.csv"
        if not gps_file.exists():
            logger.warning("GPS íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª¨ì˜ GPS ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            gps_data = self._create_mock_gps_data(len(clustered_df))
        else:
            gps_data = pd.read_csv(gps_file)
            # ì»¬ëŸ¼ëª… ì •ë¦¬
            gps_data.columns = ['time', 'latitude', 'longitude', 'height', 
                              'velocity', 'direction', 'h_accuracy', 'v_accuracy']
            # time_s ì»¬ëŸ¼ ì¶”ê°€ (ê¸°ì¡´ ì‹œìŠ¤í…œ í˜¸í™˜)
            gps_data['time_s'] = gps_data['time']
        
        # í´ëŸ¬ìŠ¤í„°ë³„ ë‚œì´ë„ ë§¤í•‘
        cluster_difficulty_map = {}
        for _, row in difficulty_df.iterrows():
            cluster_difficulty_map[row['cluster']] = {
                'difficulty_score': row['difficulty_score'],
                'difficulty_name': row['difficulty_name'],
                'difficulty_level': row['difficulty_level']
            }
        
        # ì„¸ê·¸ë¨¼íŠ¸ ë°ì´í„° ìƒì„±
        segments_data = []
        for idx, row in clustered_df.iterrows():
            cluster_id = row['cluster']
            cluster_info = cluster_difficulty_map.get(cluster_id, {})
            
            # ìœˆë„ìš° ì‹œê°„ ë²”ìœ„
            start_idx = row.get('start_idx', idx * 100)
            end_idx = row.get('end_idx', start_idx + row.get('window_size', 100))
            
            segment = {
                'segment_id': idx + 1,
                'segment_number': idx + 1,
                'start_time': start_idx / 100.0,  # 100Hz ê°€ì •
                'end_time': end_idx / 100.0,
                'duration': (end_idx - start_idx) / 100.0,
                'cluster_label': cluster_id,
                'difficulty_score': cluster_info.get('difficulty_score', 0.5),
                'vibration_rms': row.get('acc_rms_mean', 0),
                'vibration_std': row.get('acc_std_mean', 0),
                'vibration_max': row.get('acc_max_mean', 0),
                'rotation_mean': row.get('gyro_mean_mean', 0),
                'rotation_std': row.get('gyro_std_mean', 0),
                'rotation_max': row.get('gyro_max_mean', 0),
                'height_change': row.get('height_change_mean', 0),
                'velocity_mean': row.get('velocity_mean', 1.0),
                'velocity_std': row.get('velocity_std', 0.1)
            }
            
            segments_data.append(segment)
        
        # ì—£ì§€ ì •ë³´ ìƒì„± (ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ì—£ì§€ë¡œ ì²˜ë¦¬)
        edges_data = {
            'route_1': {
                'from_node': 'start',
                'to_node': 'end', 
                'segments': segments_data,
                'gps_data': gps_data,
                'difficulty_analysis': {
                    'total_segments': len(segments_data),
                    'weighted_difficulty': np.mean([s['difficulty_score'] for s in segments_data]),
                    'difficulty_level': 'ë³´í†µ',
                    'difficulty_grade': 1,
                    'cluster_ratios': {str(k): v/len(segments_data) for k, v in 
                                     pd.Series([s['cluster_label'] for s in segments_data]).value_counts().items()},
                    'avg_segment_difficulty': np.mean([s['difficulty_score'] for s in segments_data])
                }
            }
        }
        
        logger.info(f"âœ… {len(segments_data)}ê°œ ë„¤ë¹„ê²Œì´ì…˜ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±")
        return edges_data
    
    def _load_sensor_data_direct(self, data_dir: str):
        """ì„¼ì„œ ë°ì´í„° ì§ì ‘ ë¡œë”©"""
        sensor_files = {
            'accelerometer': 'Accelerometer.csv',
            'gyroscope': 'Gyroscope.csv', 
            'gravity': 'Gravity.csv',
            'linear_acceleration': 'Linear Accelerometer.csv'  # ì‹¤ì œ íŒŒì¼ëª…
        }
        
        sensor_data = {}
        
        for sensor_name, filename in sensor_files.items():
            file_path = os.path.join(data_dir, filename)
            
            if os.path.exists(file_path):
                try:
                    df = pd.read_csv(file_path)
                    logger.info(f"âœ… {sensor_name}: {len(df)} rows")
                    sensor_data[sensor_name] = df
                except Exception as e:
                    logger.error(f"âŒ {sensor_name} ë¡œë”© ì‹¤íŒ¨: {e}")
            else:
                logger.warning(f"âš ï¸  {sensor_name} íŒŒì¼ ì—†ìŒ: {file_path}")
        
        if not sensor_data:
            raise Exception("ì„¼ì„œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            
        return sensor_data
    
    def _combine_sensor_data_direct(self, sensor_data):
        """ì„¼ì„œ ë°ì´í„° ì§ì ‘ ê²°í•©"""
        if not sensor_data:
            raise Exception("ê²°í•©í•  ì„¼ì„œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        
        # ì²« ë²ˆì§¸ ì„¼ì„œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‹œì‘
        base_sensor = list(sensor_data.keys())[0]
        combined_df = sensor_data[base_sensor].copy()
        
        # ì»¬ëŸ¼ëª… ì •ê·œí™” (Timeì„ timeìœ¼ë¡œ)
        if 'Time (s)' in combined_df.columns:
            combined_df.rename(columns={'Time (s)': 'time'}, inplace=True)
            
        # ë‹¤ë¥¸ ì„¼ì„œë“¤ì˜ ë°ì´í„°ë¥¼ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
        for sensor_name, df in sensor_data.items():
            if sensor_name == base_sensor:
                continue
                
            df_copy = df.copy()
            if 'Time (s)' in df_copy.columns:
                df_copy.rename(columns={'Time (s)': 'time'}, inplace=True)
            
            # ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•© (outer joinìœ¼ë¡œ ëª¨ë“  ì‹œì  í¬í•¨)
            combined_df = pd.merge(combined_df, df_copy, on='time', how='outer', suffixes=('', f'_{sensor_name}'))
        
        # ì‹œê°„ìˆœ ì •ë ¬
        combined_df = combined_df.sort_values('time').reset_index(drop=True)
        
        # ê²°ì¸¡ê°’ ë³´ê°„
        combined_df = combined_df.interpolate(method='linear').fillna(method='bfill').fillna(method='ffill')
        
        logger.info(f"âœ… ì„¼ì„œ ë°ì´í„° ê²°í•© ì™„ë£Œ: {len(combined_df)} rows, {len(combined_df.columns)} columns")
        return combined_df
    
    def _create_mock_features(self, num_windows):
        """ì‹¤ì œ ì„¼ì„œ ë°ì´í„° ê¸°ë°˜ ëª¨ì˜ íŠ¹ì„± ìƒì„± (ë¶„ì„ í…ŒìŠ¤íŠ¸ìš©)"""
        logger.info(f"ğŸ¯ ëª¨ì˜ íŠ¹ì„± ë°ì´í„° ìƒì„±: {num_windows}ê°œ ìœˆë„ìš°")
        
        np.random.seed(42)  # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼
        
        # ì‹¤ì œ íœ ì²´ì–´ ì´ë™ì„ ëª¨ë°©í•œ íŠ¹ì„± ë°ì´í„° ìƒì„±
        features_list = []
        
        for i in range(num_windows):
            # ì§€ì—­ë³„ ë‚œì´ë„ ì‹œë®¬ë ˆì´ì…˜ (ê²½ë¡œë¥¼ ë”°ë¼ ë³€í™”)
            route_position = i / num_windows
            base_difficulty = 0.2 + 0.4 * abs(np.sin(route_position * 3.14 * 2))  # íŒŒí˜• íŒ¨í„´
            
            # ê°€ì†ë„ê³„ íŠ¹ì„± (ì§„ë™ê³¼ ì¶©ê²©)
            acc_base = 2.0 + base_difficulty * 3.0
            acc_std = 0.5 + base_difficulty * 2.0
            
            # ìì´ë¡œìŠ¤ì½”í”„ íŠ¹ì„± (íšŒì „ ì•ˆì •ì„±)
            gyro_base = 0.3 + base_difficulty * 1.2
            gyro_std = 0.2 + base_difficulty * 0.8
            
            features = {
                'window_id': i,
                'start_idx': i * 50,
                'end_idx': i * 50 + 200,
                'window_size': 200,
                
                # ê°€ì†ë„ê³„ íŠ¹ì„±
                'acc_mean_mean': acc_base + np.random.normal(0, 0.1),
                'acc_std_mean': acc_std + np.random.normal(0, 0.05),
                'acc_rms_mean': (acc_base * 1.1) + np.random.normal(0, 0.1),
                'acc_range_mean': (acc_std * 5) + np.random.normal(0, 0.2),
                'acc_max_mean': (acc_base * 1.5) + np.random.normal(0, 0.15),
                'acc_mean_diff_mean': acc_std * 0.8 + np.random.normal(0, 0.05),
                
                # ì¶•ë³„ ê°€ì†ë„
                'acc_x_std_mean': acc_std * 0.9 + np.random.normal(0, 0.03),
                'acc_y_std_mean': acc_std * 1.1 + np.random.normal(0, 0.03),
                'acc_z_std_mean': acc_std * 0.8 + np.random.normal(0, 0.03),
                
                # ìì´ë¡œìŠ¤ì½”í”„ íŠ¹ì„±
                'gyro_mean_mean': gyro_base + np.random.normal(0, 0.05),
                'gyro_std_mean': gyro_std + np.random.normal(0, 0.02),
                'gyro_rms_mean': gyro_base * 1.1 + np.random.normal(0, 0.05),
                'gyro_max_mean': gyro_base * 1.4 + np.random.normal(0, 0.08),
                
                # ì¶•ë³„ ìì´ë¡œìŠ¤ì½”í”„
                'gyro_x_std_mean': gyro_std * 0.8 + np.random.normal(0, 0.02),
                'gyro_y_std_mean': gyro_std * 1.0 + np.random.normal(0, 0.02),
                'gyro_z_std_mean': gyro_std * 0.9 + np.random.normal(0, 0.02),
                
                # Jerk íŠ¹ì„± (ì¶©ê²©)
                'jerk_mean_mean': acc_std * 1.5 + np.random.normal(0, 0.08),
                'jerk_std_mean': acc_std * 0.7 + np.random.normal(0, 0.05),
                'jerk_max_mean': acc_std * 3.0 + np.random.normal(0, 0.15),
                
                # ì¢…í•© ì§€í‘œ
                'activity_intensity_mean': (acc_base + gyro_base) * 0.8 + np.random.normal(0, 0.1),
                'stability_index_mean': 1.0 / (1.0 + acc_std + gyro_std) + np.random.normal(0, 0.02),
                
                # ë†’ì´ ë³€í™” (ì˜¤ë¥´ë§‰/ë‚´ë¦¬ë§‰ ì‹œë®¬ë ˆì´ì…˜)
                'height_change_mean': 2.0 * np.sin(route_position * 3.14 * 4) + np.random.normal(0, 0.5),
                'velocity_mean': 1.2 + np.random.normal(0, 0.2),
                'velocity_std': 0.3 + base_difficulty * 0.2 + np.random.normal(0, 0.05)
            }
            
            features_list.append(features)
        
        mock_df = pd.DataFrame(features_list)
        logger.info(f"âœ… ëª¨ì˜ íŠ¹ì„± ìƒì„± ì™„ë£Œ: {len(mock_df)}ê°œ ìœˆë„ìš°, {len(mock_df.columns)}ê°œ íŠ¹ì„±")
        
        return mock_df
    
    def _create_mock_gps_data(self, num_points):
        """ëª¨ì˜ GPS ë°ì´í„° ìƒì„±"""
        logger.info("ğŸ—ºï¸  ëª¨ì˜ GPS ë°ì´í„° ìƒì„± ì¤‘...")
        
        # ê°„ë‹¨í•œ ì§ì„  ê²½ë¡œ ìƒì„±
        start_lat, start_lon = 37.5665, 126.9780  # ì„œìš¸ ì‹œì²­ ê·¼ì²˜
        
        gps_points = []
        for i in range(num_points * 10):  # ìœˆë„ìš°ë‹¹ ì•½ 10ê°œ í¬ì¸íŠ¸
            # ë¶ìª½ìœ¼ë¡œ ì´ë™í•˜ëŠ” ì§ì„  ê²½ë¡œ
            lat = start_lat + (i * 0.00001)
            lon = start_lon + (i * 0.00001 * 0.5)
            
            gps_points.append({
                'time_s': i * 0.1,
                'latitude': lat,
                'longitude': lon,
                'height_m': 50 + np.sin(i * 0.1) * 5,  # ì•½ê°„ì˜ ê³ ë„ ë³€í™”
                'velocity_ms': 1.0 + np.random.normal(0, 0.2)
            })
        
        return pd.DataFrame(gps_points)
    
    def _save_to_database(self, edges_data):
        """ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        try:
            if not self.db_manager.connect():
                return False
            
            # ë…¸ë“œ ìƒì„± (ì‹œì‘ì , ëì )
            nodes_data = {
                'start': {
                    'latitude': 37.5665,
                    'longitude': 126.9780,
                    'name': 'ì‹œì‘ì ',
                    'type': 'start',
                    'best_match_idx': 0,
                    'best_match_distance': 0
                },
                'end': {
                    'latitude': 37.5675,
                    'longitude': 126.9790,
                    'name': 'ë„ì°©ì ', 
                    'type': 'end',
                    'best_match_idx': -1,
                    'best_match_distance': 0
                }
            }
            
            # ì €ì¥ ì‹¤í–‰
            result = self.db_manager.save_analysis_results(type('MockAnalyzer', (), {
                'nodes': nodes_data,
                'edges': edges_data
            })())
            
            return result
            
        except Exception as e:
            logger.error(f"DB ì €ì¥ ì˜¤ë¥˜: {e}")
            return False
        finally:
            self.db_manager.disconnect()
    
    def _print_summary(self, edges_data):
        """ë¶„ì„ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“Š ë„¤ë¹„ê²Œì´ì…˜ ë¶„ì„ ê²°ê³¼ ìš”ì•½")
        logger.info("=" * 60)
        
        for edge_id, edge_info in edges_data.items():
            segments = edge_info['segments']
            analysis = edge_info['difficulty_analysis']
            
            logger.info(f"\nğŸ›£ï¸  ê²½ë¡œ: {edge_id}")
            logger.info(f"   ì´ ì„¸ê·¸ë¨¼íŠ¸: {len(segments)}ê°œ")
            logger.info(f"   í‰ê·  ë‚œì´ë„: {analysis['weighted_difficulty']:.3f}")
            logger.info(f"   í´ëŸ¬ìŠ¤í„° ë¶„í¬: {analysis['cluster_ratios']}")
            
            # ì–´ë ¤ìš´ êµ¬ê°„ ê°œìˆ˜
            difficult_segments = [s for s in segments if s['difficulty_score'] > 0.6]
            logger.info(f"   ì–´ë ¤ìš´ êµ¬ê°„: {len(difficult_segments)}ê°œ ({len(difficult_segments)/len(segments)*100:.1f}%)")
            
            # ì˜ˆìƒ ë„¤ë¹„ê²Œì´ì…˜ ë©”ì‹œì§€ ìƒ˜í”Œ
            logger.info("\nğŸ—£ï¸  ì˜ˆìƒ ì•ˆë‚´ ë©”ì‹œì§€ (ìƒìœ„ 3ê°œ):")
            for i, segment in enumerate(segments[:3]):
                difficulty = segment['difficulty_score']
                if difficulty < 0.3:
                    message = f"   {i+1}. ì§ì§„ 50m - í‰íƒ„í•œ êµ¬ê°„, íœ ì²´ì–´ ì´ë™ ìš©ì´"
                elif difficulty < 0.6:
                    message = f"   {i+1}. ì§ì§„ 50m - ì•½ê°„ì˜ ì£¼ì˜ í•„ìš”, íœ ì²´ì–´ ì´ë™ ê°€ëŠ¥"
                else:
                    message = f"   {i+1}. ì§ì§„ 50m - í—˜ë‚œí•œ êµ¬ê°„, íœ ì²´ì–´ ê· í˜• ì£¼ì˜"
                logger.info(message)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ë„¤ë¹„ê²Œì´ì…˜ ë¶„ì„ í†µí•© ì‹œìŠ¤í…œ")
    print("=" * 60)
    
    # ì„¼ì„œ ë°ì´í„° í´ë” ì„¤ì •
    sensor_folder = "01 2025-11-10 18-47-59"  # í˜„ì¬ ë””ë ‰í† ë¦¬ ê¸°ì¤€
    
    # DB ì—°ê²° ì„¤ì • (í•„ìš”ì‹œ ìˆ˜ì •)
    db_config = {
        'host': '219.255.242.174',
        'database': 'grid_island', 
        'user': '5jun99',
        'password': '12341234'
    }
    
    try:
        # ë¶„ì„ ì‹¤í–‰
        runner = NavigationAnalysisRunner(db_config)
        navigation_data = runner.run_full_analysis(
            sensor_data_folder=sensor_folder,
            output_dir="results/navigation_analysis"
        )
        
        print("\nâœ… ëª¨ë“  ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ì— ë„¤ë¹„ê²Œì´ì…˜ ì„¸ê·¸ë¨¼íŠ¸ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("ğŸ—ºï¸  ì´ì œ v_navigation_segments ë·°ë¥¼ í†µí•´ ê²½ë¡œ ì•ˆë‚´ ë°ì´í„°ë¥¼ ì¡°íšŒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())